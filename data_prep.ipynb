{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as pyplot\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "import nltk.stem as stem\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = \"Data_Train.csv\"\n",
    "data_test = \"Data_Test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"Data/\" + data_train)\n",
    "df_testing = pd.read_csv(\"Data/\" + data_test)\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig, ax = pyplot.pyplot.subplots(figsize=(9,5))\n",
    "# sns.set(style=\"whitegrid\")\n",
    "# sns.countplot(ax=ax, y=\"BookCategory\", data=df_train)\n",
    "# pyplot.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sns.distplot(df_train['Price']);\n",
    "# pyplot.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('models/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text variables : Title, sysnopsis\n",
    "\n",
    "Categorical : Author, genre, bookCategory\n",
    "\n",
    "composite (categorical) : Edition\n",
    "\n",
    "real valued: Reviews, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(df_train):\n",
    "    df_train['Ratings'] = df_train.Ratings.str.split().str.get(0).str.replace(',', '').astype('int32')\n",
    "#     df_test['Ratings'] = df_test.Ratings.str.split().str.get(0).str.replace(',', '').astype('int32')\n",
    "\n",
    "    df_train['Reviews'] = df_train.Reviews.str.split().str.get(0).astype('float')\n",
    "#     df_test['Reviews'] = df_test.Reviews.str.split().str.get(0).astype('float')\n",
    "\n",
    "    df_train['Edition_type'] = df_train.Edition.str.split(',').str.get(0)\n",
    "#     df_test['Edition_type'] = df_test.Edition.str.split(',').str.get(0)\n",
    "\n",
    "    df_train['Edition_year'] = df_train.Edition.str.split().str.get(-1)\n",
    "#     df_test['Edition_year'] = df_test.Edition.str.split().str.get(-1)\n",
    "\n",
    "    df_train = df_train.drop(columns=['Edition'])\n",
    "#     df_test = df_test.drop(columns=['Edition'])\n",
    "    \n",
    "    \n",
    "\n",
    "#     y_train = df_train['Price']\n",
    "#     y_test = 0\n",
    "#     if valid:  \n",
    "#         y_test = df_test['Price']\n",
    "#         return x_train, x_test, y_train, y_test\n",
    "#     min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#     y_train = np.array(min_max_scaler.fit_transform(y_train.values.reshape(len(y_train), 1)))\n",
    "#     y_test = np.array(min_max_scaler.transform(y_test.values.reshape(len(y_test), 1)))\n",
    "   \n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = prepare_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"Data/new_data.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_train = df_train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = prepare_train_test(df_train[:int(0.8*len(df_train))], df_train[int(0.8*len(df_train)):], valid=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train = prepare_train_test(df_train, df_testing, valid=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(x_train, open('X_train.pkl', 'wb'))\n",
    "# pickle.dump(x_test, open('X_valid.pkl', 'wb'))\n",
    "# pickle.dump(y_train, open('Y_train.pkl', 'wb'))\n",
    "# pickle.dump(y_test, open('Y_valid.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(x_train, open('X_train_testing.pkl', 'wb'))\n",
    "# pickle.dump(y_train, open('Y_train_testing.pkl', 'wb'))\n",
    "# pickle.dump(x_test, open('x_testing.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_train = pickle.load(open('X_train.pkl', 'rb'))\n",
    "# x_test = pickle.load(open('X_valid.pkl', 'rb'))\n",
    "# y_train = pickle.load(open('Y_train.pkl', 'rb'))\n",
    "# y_test = pickle.load(open('Y_valid.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_train = pickle.load(open('X_train_testing.pkl', 'rb'))\n",
    "# x_test = pickle.load(open('X_testing.pkl', 'rb'))\n",
    "# y_train = pickle.load(open('Y_train_testing.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4989, 3825), (1248, 3825), (4989,), (1248,))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reg = LinearRegression()\n",
    "# reg = linear_model.Ridge(alpha=.5)\n",
    "reg = DecisionTreeRegressor(random_state=0, max_depth=100)\n",
    "# reg = linear_model.Lasso(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=100, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=0, splitter='best')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(x_train, y_train)\n",
    "# cross_val_score(regressor, boston.data, boston.target, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08269206938634542"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_one_hot_new1(data_train, data_test):\n",
    "        unique_values = list(data_train.unique())\n",
    "        unique_values = {j:i for i, j in enumerate(unique_values)}\n",
    "        total_train = []\n",
    "        total_test = []\n",
    "        for i in list(data_train):\n",
    "            val = None\n",
    "            if i in unique_values:\n",
    "                val = unique_values[i]\n",
    "            else:\n",
    "                val = len(unique_values)\n",
    "            x = [0] * (len(unique_values)+1)\n",
    "            x[val] = 1\n",
    "            total_train.append(x)\n",
    "\n",
    "        for i in list(data_test):\n",
    "            val = None\n",
    "            if i in unique_values:\n",
    "                val = unique_values[i]\n",
    "            else:\n",
    "                val = len(unique_values)\n",
    "            x = [0] * (len(unique_values)+1)\n",
    "            x[val] = 1\n",
    "            total_test.append(x)\n",
    "        return np.array(total_train), np.array(total_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genre_train, genre_test = get_one_hot_new1(df_train['Genre'], df_test['Genre'])\n",
    "\n",
    "    book_train, book_test = get_one_hot_new1(df_train['BookCategory'], df_test['BookCategory'])\n",
    "\n",
    "    author_train, author_test = get_one_hot_new1(df_train['Author'], df_test['Author'])\n",
    "\n",
    "    edition_type_train, edition_type_test = get_one_hot_new1(df_train['Edition_type'], df_test['Edition_type'])\n",
    "\n",
    "    edition_year_train, edition_year_test = get_one_hot_new1(df_train['Edition_year'], df_test['Edition_year'])\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    ratings_train = np.array(min_max_scaler.fit_transform(df_train['Ratings'].values.reshape(len(df_train['Ratings']), 1)))\n",
    "    ratings_test = np.array(min_max_scaler.transform(df_test['Ratings'].values.reshape(len(df_test['Ratings']), 1)))\n",
    "    \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    reviews_train = np.array(min_max_scaler.fit_transform(df_train['Reviews'].values.reshape(len(df_train['Reviews']), 1)))\n",
    "    reviews_test = np.array(min_max_scaler.transform(df_test['Reviews'].values.reshape(len(df_test['Reviews']), 1)))\n",
    "    \n",
    "    \n",
    "    x_train = np.concatenate((ratings_train, reviews_train, genre_train, book_train, author_train, edition_type_train, edition_year_train, synopsis_train), axis=1)\n",
    "\n",
    "    x_test = np.concatenate((ratings_test, reviews_test, genre_test, book_test, author_test, edition_type_test, edition_year_test, synopsis_test), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = stem.PorterStemmer()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def stem_stop(input_text):\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input_text.lower())\n",
    "    tokens = [x for x in tokens if not x in stop_words]\n",
    "    tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
    "    return tokens_stemmed\n",
    "\n",
    "sentences_train = list(df_train['Synopsis'])\n",
    "#     sentences_test = list(df_test['Synopsis'])\n",
    "\n",
    "sentences_train = [stem_stop(i) for i in sentences_train]\n",
    "#     sentences_test = [stem_stop(i) for i in sentences_test]\n",
    "\n",
    "def get_mean_vector(word2vec_model, words):\n",
    "    words = [word for word in words if word in word2vec_model.vocab]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(word2vec_model[words], axis=0)\n",
    "    else:\n",
    "        return np.zeroes(300)\n",
    "\n",
    "synopsis_train = np.array([get_mean_vector(model, i) for i in sentences_train])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
